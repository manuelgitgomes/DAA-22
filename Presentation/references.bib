
@article{Ali2019,
AUTHOR = {I. Ali and O. Suominen and A. Gotchev and E. Morales},
TITLE = {{Methods for Simultaneous Robot-World-Hand–Eye Calibration: A Comparative Study}},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2837},
ISSN = {1424-8220},
DOI = {10.3390/s19122837}
}

@article{Tabb2017,
author = {A. Tabb and K. Yousef},
doi = {10.1007/s00138-017-0841-7},
isbn = {0013801708},
issn = {0932-8092},
journal = {Machine Vision and Applications},
keywords = {Calibration,Hand-eye,Reconstruction,Robot},
month = {aug},
number = {5-6},
pages = {569--590},
publisher = {Springer Berlin Heidelberg},
title = {{Solving the robot-world hand-eye(s) calibration problem with iterative methods}},
volume = {28},
year = {2017}
}


@article{Park1994,
    author={F. {Park} and B. {Martin}},
    journal={IEEE Trans. on Robotics and Automation},
    title={{Robot sensor calibration: solving AX=XB on the Euclidean group}},
    year={1994},
    volume={10},
    number={5},
    pages={717-721},
    doi={10.1109/70.326576},
}


@article{Vasconcelos2012,
    author={F. {Vasconcelos} and J. {Barreto} and U. {Nunes}},
    journal={IEEE Trans. on Pattern Analysis and Machine Intelligence},
    title={A Minimal Solution for the Extrinsic Calibration of a Camera and a Laser-Rangefinder},
    year={2012},
    volume={34},
    number={11},
    pages={2097-2107},
    month={Nov}
}


@article{Pereira2016,
    title = "Self calibration of multiple LiDARs and cameras on autonomous vehicles",
    journal = "Robotics and Autonomous Systems",
    volume = "83",
    pages = "326 - 337",
    year = "2016",
    month = "Sep",
    author = "M. Pereira and D. Silva and V. Santos and P. Dias"
}


@inproceedings{Almeida2012,
    author="M. Almeida and P. Dias and M. Oliveira and V. Santos",
    title="3D-2D Laser Range Finder Calibration Using a Conic Based Geometry Shape",
    booktitle="Image Analysis and Recognition",
    month="Jun",
    year="2012",
    pages="312--319"
}


@INPROCEEDINGS{Zhang2004,
author={Q. {Zhang} and R. {Pless}},
booktitle={2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
title={Extrinsic calibration of a camera and laser range finder (improves camera calibration)},
year={2004},
volume={3},
number={},
pages={2301-2306 vol.3},
doi={10.1109/IROS.2004.1389752},
ISSN={},
month={Sep.}
}

@ARTICLE{Zhuang1994,
    author={H. {Zhuang} and Z. {Roth} and R. {Sudhakar}},
    journal={IEEE Trans. on Robotics and Automation},
    title={{Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX=YB}},
    year={1994},
    volume={10},
    number={4},
    pages={549-554},
    doi={10.1109/70.313105},
    ISSN={2374-958X},
    month={Aug},
}

@ARTICLE{Dornaika1998,
author={F. {Dornaika} and R. {Horaud}},
journal={IEEE Trans. on Robotics and Automation},
title={Simultaneous robot-world and hand-eye calibration},
year={1998},
volume={14},
number={4},
pages={617-622},
doi={10.1109/70.704233},
ISSN={2374-958X},
month={Aug},
}

@article{Dornaika1995,
    author = {R. Horaud and F. Dornaika},
    title ={Hand-Eye Calibration},
    journal = {The Int. Journal of Robotics Research},
    volume = {14},
    number = {3},
    pages = {195-210},
    year = {1995},
    doi = {10.1177/027836499501400301},
}

@INPROCEEDINGS{Hirsh2001,
author={R. L. {Hirsh} and G. N. {DeSouza} and A. C. {Kak}},
booktitle={Proceedings 2001 ICRA. IEEE Int. Conf. on Robotics and Automation (Cat. No.01CH37164)},
title={An iterative approach to the hand-eye and base-world calibration problem},
year={2001},
volume={3},
number={},
pages={2171-2176 vol.3},
doi={10.1109/ROBOT.2001.932945},
ISSN={1050-4729},
month={May},
}

@article{Li2010,
    title={{Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product}},
    author={A. Liz and L. Wang and D. Wu},
    journal={Int. Journal of the Physical Sciences},
    volume={5},
    number={10},
    pages={1530--1536},
    year={2010}
}

@article{Shah2013,
    author = {M. Shah},
    title = {{Solving the Robot-World/Hand-Eye Calibration Problem Using the Kronecker Product}},
    journal = {Journal of Mechanisms and Robotics},
    volume = {5},
    number = {3},
    year = {2013},
    month = {06},
    issn = {1942-4302},
    doi = {10.1115/1.4024473},
}




@inproceedings{Haeselich2012,
    author={M. Häselich and R. Bing and D. Paulus},
    booktitle={2012 IEEE Int. Conf. on Emerging Signal Processing Applications},
    title={Calibration of multiple cameras to a 3D laser range finder},
    year={2012},
    volume={},
    number={},
    pages={25-28},
    month={Jan}
}

@inproceedings{Chen2016,
    author={Z. Chen and X. Yang and C. Zhang and S. Jiang},
    booktitle={2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
    title={Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature},
    year={2016},
    volume={},
    number={},
    pages={448-453},
    month={Oct}
}


@inproceedings{Schmidt2001,
    author = {J. Schmidt and H. Niemann},
    year = {2001},
    month = {01},
    title = {Using Quaternions for Parametrizing 3-D Rotations in Unconstrained Nonlinear Optimization},
    booktitle = {Vision, Modeling, and Visualization}
}

@article{Kassir2010,
    title={Reliable Automatic Camera-Laser Calibration},
    author={A. Kassir and T. Peynot},
    year={2010},
    journal = {Proc. Australasian Conf. on Robotics \& Automation}
}


@Inbook{Pradeep2014,
author="V. Pradeep and K. Konolige and E. Berger",
title="Calibrating a Multi-arm Multi-sensor Robot: A Bundle Adjustment Approach",
bookTitle="Experimental Robotics: The 12th Int. Symposium on Experimental Robotics",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="211--225",
isbn="978-3-642-28572-1",
doi="10.1007/978-3-642-28572-1_15"
}


@article{Plaza2019,
author = "P. Marin-Plaza and A. Hussein and D. Martin and A. de la Escalera",
title = "iCab Use Case for ROS-based Architecture",
journal = "Robotics and Autonomous Systems",
volume = "118",
pages = "251 - 262",
year = "2019",
issn = "0921-8890",
doi = "10.1016/j.robot.2019.04.008"
}
%censor all of this:
@INPROCEEDINGS{Santos2010,
author={V. {Santos} and J. {Almeida} and E. {Ávila} and D. {Gameiro} and M. {Oliveira} and R. {Pascoal} and  R. {Sabino} and P. {Stein}},
booktitle={13th Int. IEEE Conf. on Intelligent Transpor tation Systems},
title={ATLASCAR - technologies for a computer assisted driving system, on board a common automobile},
year={2010},
volume={},
number={},
pages={1421-1427},
keywords={automated highways;road traffic;state feedback;ATLASCAR;computer assisted driving system;common automobile;intelligent vehicles;robust information;proper feedback;active safety;vehicle autonomy;driver assistance;scientific solutions;data gathering;Sensors;Cameras;Vehicles;Driver circuits;Three dimensional displays;Image color analysis;Roads},
doi={10.1109/ITSC.2010.5625031},
ISSN={2153-0017},
month={Sep.}
}

@conference {Quigley2009,
	author = {M. Quigley and K. Conley and  B. Gerkey and J. Faust and T. Foote and J Leibs and R. Wheeler and A. Ng},
	title = {{ROS: an open-source Robot Operating System}},
	booktitle = {ICRA Workshop on Open Source Software},
	year = {2009}
}

@INPROCEEDINGS{Su2016,
author={ R. {Su} and J. {Zhong} and Q. {Li} and S. {Qi} and H. {Zhang} and T. {Wang}},
booktitle={2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC)},
title={An automatic calibration system for binocular stereo imaging},
year={2016},
volume={},
number={},
pages={896-900},
abstract={In this paper, an automatic calibration system for binocular stereo imaging based on Zhang's 2D flat calibration method is proposed. Firstly, the interface of system is designed. After images being collected, the result of Harris corners detection extracted from the 2D flat are displayed on the interface, meanwhile, the system begins to compute intrinsic and extrinsic parameters of the stereo camera and then optimizes them by maximum likelihood estimate. Finally, mean reprojection error and the visualization of camera intrinsic parameters shown on the interface is convenient for users to analyze factors of effecting stereo camera calibration.The results appear that the proposed system not only operates easily but also obtains high precision of calibration, which improves the traditional method of camera calibration.},
keywords={calibration;cameras;computer vision;edge detection;maximum likelihood estimation;optimisation;stereo image processing;automatic calibration system;binocular stereo imaging;Zhang 2D flat calibration method;system interface design;Harris corner detection;parameter optimization;maximum likelihood estimate;mean reprojection error;camera intrinsic parameter visualization;stereo camera calibration;Calibration;Automatic;binocular stereo imaging;Zhang's 2D flat calibration;camera intrinsic parameters},
doi={10.1109/IMCEC.2016.7867340},
ISSN={},
month={Oct}
}


@INPROCEEDINGS{Wu2015,
author={L. {Wu} and B. {Zhu}},
booktitle={2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA)},
title={Binocular stereovision camera calibration},
year={2015},
volume={},
number={},
pages={2638-2642},
abstract={In view of the binocular stereo vision calibration problem, this paper offers a method of camera calibration which is between the traditional calibration and the self-calibration. To compare with traditional methods, set the internal parameters distortion parameters and external parameters calibrated respectively. First of all, working out the internal parameters by Zhang Z.Y plane calibration method; secondly, calculating the distortion parameters based on the radial distortion lens; finally, discussing the calibration methods of external parameters in the case of the camera is moving, to avoid the problem of recalibration when the camera is moving in original calibration environment. The experiment according to the template principle of traditional calibration, combined with binocular stereovision model, verifies the feasibility of this separation calibration method.},
keywords={calibration;cameras;lenses;stereo image processing;visual perception;binocular stereovision camera calibration;binocular stereo vision calibration problem;internal parameter distortion parameter;external parameter calibration;plane calibration method;radial distortion lens;Cameras;Calibration;Robot vision systems;Nonlinear distortion;stereovision;camera calibration;separation calibration;radial distortion;recalibration},
doi={10.1109/ICMA.2015.7237903},
ISSN={2152-7431},
month={Aug}}


@INPROCEEDINGS{Mueller2017,
author={G. {Mueller} and H. {Wuensche}},
booktitle={2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC)},
title={Continuous stereo camera calibration in urban scenarios},
year={2017},
volume={},
number={},
pages={1-6},
abstract={The quality of a 3D reconstruction obtained from stereo images depends on the accuracy of the stereo camera calibration. Continuous online camera calibration is key to enable a long maintenance-free operation of autonomous vehicles. Conventional algorithms for online calibration usually assume a static world. This, however, is often violated in urban scenarios. In this paper an algorithm is presented that determines the twelve degrees-of-freedom (12-DoF) extrinsic calibration of a stereo camera system in dynamic, urban scenarios. An Extended Kalman Filter (EKF) continuously estimates the extrinsic stereo camera calibration by tracking the position of salient points in 3D space that are observed in both cameras. The EKF predicts the 3D position of tracked points based on the estimated calibration and the vehicle's motion, which is estimated using an inertial navigation system (INS) and odometry sensors. However, only static points can be reliable predicted. A convolutional neural network (CNN) is applied to segment camera images on a pixel level. These segmented images are further converted to binary images labelling static and potentially dynamic pixels. Only salient points which are labeled as static are retained for estimation of the stereo camera calibration. While the stereo camera's extrinsic parameters are only observable under transformation and two independent rotations of the vehicle, we present a robust filter update scheme, that enables estimation of the 12-DoF extrinsic stereo camera calibration even in the absence of significant rotations. Test on urban roads show that minor road imperfections are sufficient to estimate all 12-DoF extrinsic parameters over time.},
keywords={calibration;cameras;computer vision;image reconstruction;image segmentation;image sensors;inertial navigation;Kalman filters;mobile robots;neural nets;nonlinear filters;robot vision;stereo image processing;tracking;static points;segment camera images;static pixels;potentially dynamic pixels;salient points;12-DoF extrinsic stereo camera calibration;12-DoF extrinsic parameters;continuous stereo camera calibration;urban scenarios;stereo images;continuous online camera calibration;long maintenance-free operation;online calibration;stereo camera system;dynamic scenarios;estimated calibration;odometry sensors;Cameras;Calibration;Heuristic algorithms;Robot vision systems;Three-dimensional displays;Vehicle dynamics;Prediction algorithms},
doi={10.1109/ITSC.2017.8317675},
ISSN={2153-0017},
month={Oct}}


@ARTICLE{Dinh2019,
author={V. {Dinh} and T. {Nguyen} and J. {Jeon}},
journal={IEEE Trans. on Image Processing},
title={Rectification Using Different Types of Cameras Attached to a Vehicle},
year={2019},
volume={28},
number={2},
pages={815-826},
abstract={The rectification process is a compulsory step in stereo matching computation. To obtain depth information, stereo camera systems are often installed in vehicles for outdoor and street-related applications, including vehicle and pedestrian detection, lane detection, and traffic sign recognition. In this paper, we propose a rectification method that uses currently available front- and rear-view vehicle cameras to produce rectified stereo images. The proposed method can be employed with different types of cameras that have varying focal lengths. In addition, this method tolerates the problem of camera alignment variation from normal stereo camera systems. To achieve this, a compensation method for different focal lengths and the estimation of image relationships are introduced. The experimental results demonstrate that the proposed method can operate robustly and accurately with different kinds of stereo images and significantly outperforms a state-of-the-art rectification method.},
keywords={cameras;compensation;image matching;rectification;stereo image processing;front-view vehicle cameras;rectified stereo imaging;lane detection;pedestrian detection;stereo matching computation;compensation method;normal stereo camera systems;camera alignment variation;rear-view vehicle cameras;traffic sign recognition;Cameras;Calibration;Transmission line matrix methods;Sociology;Statistics;Robustness;Indexes;Uncalibrated rectification;different cameras;SIFT matching},
doi={10.1109/TIP.2018.2870930},
ISSN={1057-7149},
month={Feb}}


@INPROCEEDINGS{Ling2016,
author={Y. {Ling} and S. {Shen}},
booktitle={2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)},
title={High-precision online markerless stereo extrinsic calibration},
year={2016},
volume={},
number={},
pages={1771-1778},
abstract={Stereo cameras and dense stereo matching algorithms are core components for many robotic applications due to their abilities to directly obtain dense depth measurements and their robustness against changes in lighting conditions. However, the performance of dense depth estimation relies heavily on accurate stereo extrinsic calibration. In this work, we present a real-time markerless approach for obtaining high-precision stereo extrinsic calibration using a novel 5-DOF (degrees-of-freedom) and nonlinear optimization on a manifold, which captures the observability property of vision-only stereo calibration. Our method minimizes epipolar errors between spatial per-frame sparse natural features. It does not require temporal feature correspondences, making it not only invariant to dynamic scenes and illumination changes, but also able to run significantly faster than standard bundle adjustment-based approaches. We introduce a principled method to determine if the calibration converges to the required level of accuracy, and show through online experiments that our approach achieves a level of accuracy that is comparable to offline marker-based calibration methods. Our method refines stereo extrinsic to the accuracy that is sufficient for block matching-based dense disparity computation. It provides a cost-effective way to improve the reliability of stereo vision systems for long-term autonomy.},
keywords={calibration;estimation theory;image matching;nonlinear programming;observability;robot vision;stereo image processing;vision-only stereo calibration;observability property;nonlinear optimization;dense depth estimation;robotic application;stereo matching algorithm;stereo camera;real-time markerless approach;high-precision stereo extrinsic calibration;Calibration;Cameras;Robot vision systems;Real-time systems},
doi={10.1109/IROS.2016.7759283},
ISSN={2153-0866},
month={Oct}}


@INPROCEEDINGS{Kwon2018,
author={Y. {Kwon} and J. {Jang} and O. {Choi}},
booktitle={2018 18th Int. Conf. on Control, Automation and Systems (ICCAS)},
title={Automatic sphere detection for extrinsic calibration of multiple RGBD cameras},
year={2018},
volume={},
number={},
pages={1451-1454},
abstract={RGBD cameras provide color and depth images at a high speed, so they are becoming increasingly important in many computer vision applications. With the color and depth information, it is easy to build a textured 3D model of a scene without effort on correspondence search. To combine depth data from multiple RGBD cameras in a unified coordinate system, the cameras need to be extrinsically calibrated based on 3D correspondences of scene features. To maximize the scene features simultaneously observed from different viewpoints, a spherical calibration object is widely used. In this paper, we propose a method for automatically detecting a spherical object in an RGB image. Since the detection relies only on RGB information, the proposed method has a potential to be extended to conventional multi-view calibration. Experimental results show that the proposed method accurately detects spherical objects in a cluttered environment.},
keywords={calibration;cameras;computer vision;image colour analysis;image reconstruction;image registration;image sensors;image texture;object detection;multiview calibration;depth images;automatic sphere detection;RGB information;RGB image;spherical calibration object;scene features;unified coordinate system;multiple RGBD cameras;depth data;textured 3D model;depth information;computer vision applications;Cameras;Image color analysis;Calibration;Three-dimensional displays;Detectors;Solid modeling;Computer vision;RGBD camera;camera calibration;sphere detection;extrinsic parameter},
doi={},
ISSN={},
month={Oct}}

@INPROCEEDINGS{Yang2018,
author={L. {Yang} and Q. {Cao} and M. {Lin} and H. {Zhang} and Z. {Ma}},
booktitle={2018 4th Int. Conf. on Control, Automation and Robotics (ICCAR)},
title={Robotic hand-eye calibration with depth camera: A sphere model approach},
year={2018},
volume={},
number={},
pages={104-110},
abstract={This paper presents a new depth-based hand-eye calibration method to find the transformation between depth camera and robot wrist. Owing to the true hand-eye transformation is hard to obtained, indirect measurements depending on robot movements and calibration tools is adopted. In our method, A sphere, which has simple and elegant parameterized presentation, is used as calibration tool. The points cloud acquired from the depth camera viewing the calibration tool are aligned to a virtual sphere model by RANSAC to find the translation part of the homogeneous transformation between depth camera and calibration tool. The rotational part of the transformation is ambiguous thus eliminated by reformulating the hand-eye calibration problem. Then a non-linear optimization method is proposed to solve the hand-eye rotation and translation simultaneously. Due to the ground truth hand-eye transformation is not available, a quantitative error measurement is adopted to measure the accuracy.},
keywords={calibration;cameras;control engineering computing;dexterous manipulators;robot vision;calibration tool;elegant parameterized presentation;depth camera;virtual sphere model;homogeneous transformation;hand-eye calibration problem;hand-eye rotation;ground truth hand-eye transformation;robotic hand-eye calibration;sphere model approach;hand-eye calibration method;robot wrist;robot movements;Calibration;Cameras;Robot kinematics;Robot vision systems;Three-dimensional displays;Quaternions;hand-eye calibration;geometric alignment;depth cameras;non-linear optimization},
doi={10.1109/ICCAR.2018.8384652},
ISSN={},
month={April}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
RGBD CAMERAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@INPROCEEDINGS{Khan2016,
author={A. {Khan} and G. {Camarasa} and L. {Sun} and J. P. {Siebert}},
booktitle={2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO)},
title={On the calibration of active binocular and RGBD vision systems for dual-arm robots},
year={2016},
volume={},
number={},
pages={1960-1965},
doi={10.1109/ROBIO.2016.7866616},
ISSN={},
month={Dec}
}

@ARTICLE{Basso2018,
author={F. {Basso} and E. {Menegatti} and A. {Pretto}},
journal={IEEE Trans. on Robotics},
title={Robust Intrinsic and Extrinsic Calibration of RGB-D Cameras},
year={2018},
volume={34},
number={5},
pages={1315-1332},
abstract={Color-depth cameras (RGB-D cameras) have become the primary sensors in most robotics systems, from service robotics to industrial robotics applications. Typical consumer-grade RGB-D cameras are provided with a coarse intrinsic and extrinsic calibration that generally does not meet the accuracy requirements needed by many robotics applications [e.g., highly accurate three-dimensional (3-D) environment reconstruction and mapping, high precision object recognition, localization, etc.]. In this paper, we propose a human-friendly, reliable, and accurate calibration framework that enables to easily estimate both the intrinsic and extrinsic parameters of a general color-depth sensor couple. Our approach is based on a novel two components error model. This model unifies the error sources of RGB-D pairs based on different technologies, such as structured-light 3-D cameras and time-of-flight cameras. Our method provides some important advantages compared to other state-of-the-art systems: It is general (i.e., well suited for different types of sensors), based on an easy and stable calibration protocol, provides a greater calibration accuracy, and has been implemented within the robot operating system robotics framework. We report detailed experimental validations and performance comparisons to support our statements.},
keywords={calibration;cameras;image colour analysis;image reconstruction;image sensors;industrial robots;object recognition;service robots;stereo image processing;components error model;RGB-D pairs;calibration protocol;color-depth sensor couple;structured-light 3D cameras;robust intrinsic calibration;three-dimensional environment reconstruction;three-dimensional environment mapping;human-friendly calibration framework;high precision object recognition;environment reconstruction;coarse intrinsic calibration;typical consumer-grade RGB-D cameras;industrial robotics applications;service robotics;color-depth cameras;system robotics framework;state-of-the-art systems;time-of-flight cameras;Cameras;Calibration;Robot vision systems;Sensor systems;Camera calibration;camera pairs;depth cameras},
doi={10.1109/TRO.2018.2853742},
ISSN={1552-3098},
month={Oct}}

@INPROCEEDINGS{Qiao2013,
author={Y. {Qiao} and B. {Tang} and Y. {Wang} and L. {Peng}},
booktitle={2013 Int. Conf. on Computational Problem-Solving (ICCP)},
title={A new approach to self-calibration of hand-eye vision systems},
year={2013},
volume={},
number={},
pages={253-256},
doi={10.1109/ICCPS.2013.6893596},
ISSN={},
month={Oct}}


@INPROCEEDINGS{Zhang2011,
author={C. {Zhang} and Z. {Zhang}},
booktitle={2011 IEEE Int. Conf. on Multimedia and Expo},
title={Calibration between depth and color sensors for commodity depth cameras},
year={2011},
volume={},
number={},
pages={1-6},
abstract={Commodity depth cameras have created many interesting new applications in the research community recently. These applications often require the calibration information between the color and the depth cameras. Traditional checkerboard based calibration schemes fail to work well for the depth camera, since its corner features cannot be reliably detected in the depth image. In this paper, we present a maximum likelihood solution for the joint depth and color calibration based on two principles. First, in the depth image, points on the checker board shall be co-planar, and the plane is known from color camera calibration. Second, additional point correspondences between the depth and color images may be manually specified or automatically established to help improve calibration accuracy. Uncertainty in depth values has been taken into account systematically. The proposed algorithm is reliable and accurate, as demonstrated by extensive experimental results on jimulated and real-world examples.},
keywords={calibration;cameras;image sensors;maximum likelihood estimation;depth sensor;color sensor;commodity depth camera;depth image;maximum likelihood solution;checker board;color camera calibration;Indexes;Frequency locked loops;depth camera;calibration},
doi={10.1109/ICME.2011.6012191},
ISSN={1945-788X},
month={July}}

@ARTICLE{Chen2019,
author={G. {Chen} and G. {Cui} and Z. {Jin} and F. {Wu} and X. {Chen}},
journal={IEEE Sensors Journal},
title={Accurate Intrinsic and Extrinsic Calibration of RGB-D Cameras With GP-Based Depth Correction},
year={2019},
volume={19},
number={7},
pages={2685-2694},
abstract={In recent years, more and more robots have been equipped with low-cost RGB-D sensors, such as Microsoft Kinect and Intel Realsense, for safe navigation and active interaction with objects and people. In order to obtain more accurate and reliable fused color and depth information (coloured point clouds), not only the intrinsic and extrinsic parameters of color and depth sensor should be precisely calibrated but also the external corrections of depth measurements are required. In this paper, using motion capture system, we propose a reliable calibration framework that enables the precise estimation of the intrinsic and extrinsic parameters of RGB-D sensors and provide a model-free depth calibration method based on heteroscedastic Gaussian processes. Compared with the existing depth correction techniques, our method can simultaneously estimate the mean and variance of the depth error at different measurement distances, i.e., the probability distribution of the depth error relative to the measured distance, which is essential in the state estimation problems. To verify the effectiveness of our approach, we conduct a thorough qualitative and quantitative analysis of the major steps of our calibration method, and compare our experimental results with other related work. Furthermore, we demonstrate an experiment about the overall improvement of visual SLAM with a Kinect device calibrated by our calibration technique.},
keywords={calibration;cameras;distance measurement;Gaussian processes;image colour analysis;image motion analysis;measurement errors;motion measurement;parameter estimation;reliability;state estimation;statistical distributions;Microsoft Kinect;Intel Realsense;coloured point clouds;depth sensor;depth measurements;motion capture system;model-free depth calibration method;heteroscedastic Gaussian processes;state estimation problems;navigation;distance measurement;fused color reliability;extrinsic calibration parameter technique;intrinsic calibration parameter technique;RGB-D camera sensors;GP-based depth correction techniques;probability distribution;visual SLAM;Calibration;Cameras;Distortion;Measurement uncertainty;Distortion measurement;Robot vision systems;RGB-D cameras;calibration;motion capture system},
doi={10.1109/JSEN.2018.2889805},
ISSN={1530-437X},
month={April}}

@ARTICLE{Rehder2016,
author={J. {Rehder} and R. {Siegwart} and P. {Furgale}},
journal={IEEE Trans. on Robotics},
title={A General Approach to Spatiotemporal Calibration in Multisensor Systems},
year={2016},
volume={32},
number={2},
pages={383-398},
abstract={With growing demands for accuracy in sensor fusion, increasing attention is being paid to temporal offsets as a source of deterministic error when processing data from multiple devices. Established approaches for the calibration of temporal offsets exploit domain-specific heuristics of common sensor suites and utilize simplifications to circumvent some of the challenges arising when both temporal and spatial parameters are not accurately known a priori. These properties make it difficult to generalize the work to other applications or different combinations of sensors. This work presents a general and principled approach to joint estimation of temporal offsets and spatial transformations between sensors. Our framework exploits recent advances in continuous-time batch estimation and thus exists within the rigorous theoretical framework of maximum likelihood estimation. The derivation is presented without relying on unique properties of specific sensors and, therefore, represents the first general technique for temporal calibration in robotics. The broad applicability of this approach is demonstrated through spatiotemporal calibration of a camera with respect to an inertial measurement unit as well as between a stereo camera and a laser range finder. The method is shown to be more repeatable and accurate than the current state of the art, estimating spatial displacements to millimeter precision and temporal offsets to a fraction of the fastest measurement interval.},
keywords={maximum likelihood estimation;robots;sensor fusion;spatiotemporal calibration;multisensor system;sensor fusion;domain-specific heuristics;spatial transformation;temporal offsets;continuous-time batch estimation;maximum likelihood estimation;robotics;inertial measurement unit;stereo camera;laser range finder;spatial displacement;Calibration;Cameras;Synchronization;Estimation;Clocks;Robot vision systems;Multisensor calibration;sensor fusion;sensor synchronization},
doi={10.1109/TRO.2016.2529645},
ISSN={1552-3098},
month={April}}


@INPROCEEDINGS{Furgale2013,
  author={P. {Furgale} and J. {Rehder} and R. {Siegwart}},
  booktitle={2013 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems},
  title={Unified temporal and spatial calibration for multi-sensor systems},
  year={2013},
  volume={},
  number={},
  pages={1280-1286}
}

@INPROCEEDINGS{Liao2017,
author={Y. {Liao} and G. {Li} and Z. {Ju} and H. {Liu} and D. {Jiang}},
booktitle={2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM)},
title={Joint kinect and multiple external cameras simultaneous calibration},
year={2017},
volume={},
number={},
pages={305-310},
abstract={Camera calibration applications are used in many places, such as visual estimation, 3D reconstruction and object tracking. Over the past few decades, many methods have been proposed and applied to these areas. However, with the diversification of sensor technology, the problem of multi-sensor joint calibration becomes more and more obvious. In this paper, we propose a new method that can simultaneously calibrate relative poses of three external cameras and a Kinect. And the final cost function is optimized for joint calibration. In addition, the practical platform has also tested the method, and the experimental results show that the accuracy of the proposed method significantly improved.},
keywords={calibration;cameras;image reconstruction;image sensors;object tracking;stereo image processing;3D reconstruction;multisensor joint calibration;sensor technology;object tracking;visual estimation;camera calibration applications;multiple external cameras simultaneous calibration;joint kinect;Calibration;Image color analysis;Distortion;Robot sensing systems;Mechatronics;Color;Kinect Calibration;Camera Calibration;Computer Vision},
doi={10.1109/ICARM.2017.8273179},
ISSN={},
month={Aug}}

@INPROCEEDINGS{Gao2010,
author={ {D. Gao} and J. {Duan} and {X. Yang} and B. {Zheng}},
booktitle={2010 8th World Congress on Intelligent Control and Automation},
title={A method of spatial calibration for camera and radar},
year={2010},
volume={},
number={},
pages={6211-6215},
abstract={The technique of multi-sensor fusion has already been used widely in intelligent vehicle environment perception. The spatial calibration of the radar and camera is the basis for road detection with information fusion real-timely. So, a method of spatial calibration of radar and the camera has been proposed to implement the spatial calibration of the two sensors. The non-linear distortion of the camera is considered in the paper. Coordinate transformation between the coordinates relevant to the radar and camera is introduced as the constraints. After transforming the feature between the related coordinates of image and the relative coordinates of radar, the calibration parameters are determined with the least square error function. Experiment results indicate that this method is simple and easy to realize, which has the high accuracy that satisfy requirement of the system.},
keywords={calibration;cameras;object detection;radar;road vehicles;sensor fusion;spatial calibration;camera;radar;multi-sensor fusion;intelligent vehicle environment perception;road detection;information fusion;nonlinear distortion;Cameras;Laser radar;Calibration;Radar imaging;Charge coupled devices;Sensors;camera;laser radar;multi-sensor data fusion;spatial calibration},
doi={10.1109/WCICA.2010.5554411},
ISSN={},
month={July}}

@INPROCEEDINGS{Lee2017,
author={G. {Lee} and J. {Lee} and S. {Park}},
booktitle={2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
title={Calibration of VLP-16 Lidar and multi-view cameras using a ball for 360 degree 3D color map acquisition},
year={2017},
volume={},
number={},
pages={64-69},
abstract={Calibration between Lidar sensor and RGB cameras can be applied to various fields such as object recognition and tracking, 2D-3D mapping, and simultaneous localization and mapping (SLAM). Different methods for calibrating Lidar sensor and RGB cameras have been proposed using special 3D markers or calibration patterns. However, most of these methods have disadvantages of longer processing time, and various experimental constraints such as entire calibration pattern must appear within the scan range of the Lidar. In this paper, we propose a simple and fast calibration method between a Lidar sensor and multiple RGB cameras using a sphere object.},
keywords={calibration;cameras;image colour analysis;object recognition;optical radar;robot vision;SLAM (robots);360 degree 3D color map acquisition;object recognition;simultaneous localization;mapping;special 3D markers;calibration patterns;calibration method;VLP-16 Lidar;multiview cameras;object tracking;Three-dimensional displays;Cameras;Laser radar;Two dimensional displays;Calibration;Image color analysis;Autonomous vehicles},
doi={10.1109/MFI.2017.8170408},
ISSN={},
month={Nov}}

@ARTICLE{Cousins2010,
author={S. {Cousins}},
journal={IEEE Robotics Automation Magazine},
title={ROS on the PR2 [ROS Topics]},
year={2010},
volume={17},
number={3},
pages={23-25},
abstract={In the last issue of IEEE Robotics & Automation Magazine, we talked about a number of different robots running the robot operating system (ROS). In this issue, we are going to focus on one in particular, the PR2. Willow Garage announced in May the 11 institutions that will get to use a PR2 for free for two years through its PR2 Beta Program (Figure 1). The PR2s all run ROS, and the institutions have all committed to making significant open-source contributions back to the ROS community. ROS has been gaining traction at an astonishing rate since its release in January, but having 11 PR2s at top institutions will accelerate that progress even further. We will give a brief overview of what the PR2 Beta Program recipients are planning to do, and then dive into a concrete example of the use of ROS on the PR2.},
keywords={control engineering computing;operating systems (computers);robots;ROS;PR2;IEEE robotics;automation magazine;robot operating system;open-source contributions},
doi={10.1109/MRA.2010.938502},
ISSN={1070-9932},
month={Sep.}}

@INPROCEEDINGS{Le2009,
author={Q. V. {Le} and A. Y. {Ng}},
booktitle={2009 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems},
title={Joint calibration of multiple sensors},
year={2009},
volume={},
number={},
pages={3651-3658},
abstract={Many calibration methods calibrate a pair of sensors at a time. For robotic systems with many sensors, they are often time-consuming to use, and can also lead to inaccurate results. In this paper, we combine a number of ideas in the literature to derive a unified framework that jointly calibrates many sensors a large system. Key to our approach are (i) grouping sensors to produce 3D data, thereby providing a unifying formalism that allows us to jointly calibrate all of the groups at the same, (ii) using a variety of geometric constraints to perform the calibration, and (iii) sharing sensors between groups to increase robustness. We show that this gives a simple method that is easily applicable to calibrating large systems. Our experiments show that this method not only reduces calibration error, but also requires less human time.},
keywords={calibration;manipulators;sensor fusion;joint sensor calibration;robotic systems;grouping sensors;geometric constraints;sharing sensors;Calibration;Robot sensing systems;Sensor systems;Cameras;Intelligent sensors;Robot vision systems;Robustness;Humans;Optimization methods;Intelligent robots},
doi={10.1109/IROS.2009.5354272},
ISSN={2153-0858},
month={Oct}}

@INPROCEEDINGS{Foote2013,
author={T. {Foote}},
booktitle={2013 IEEE Conf. on Technologies for Practical Robot Applications (TePRA)},
title={tf: The transform library},
year={2013},
volume={},
number={},
pages={1-6},
abstract={The tf library was designed to provide a standard way to keep track of coordinate frames and transform data within an entire system such that individual component users can be confident that the data is in the coordinate frame that they want without requiring knowledge of all the coordinate frames in the system. During early development of the Robot Operating System (ROS), keeping track of coordinate frames was identified as a common pain point for developers. The complexity of this task made it a common place for bugs when developers improperly applied transforms to data. The problem is also a challenge due to the often distributed sources of information about transformations between different sets of coordinate frames. This paper will explain the complexity of the problem and distill the requirements. Then it will discuss the design of the tf library in relation to the requirements. A few use cases will be presented to demonstrate successful deployment of the library. And powerful extensions to the core capabilities such as being able to transform data in time as well as in space.},
keywords={libraries;operating systems (computers);robots;transform library;tf library;robot operating system;ROS;coordinate frames;Irrigation;Accuracy},
doi={10.1109/TePRA.2013.6556373},
ISSN={2325-0534},
month={April}}

@inproceedings{Hornegger1999,
author = {J. Hornegger and C. Tomasi},
year = {1999},
month = {02},
pages = {640 - 647 vol.1},
title = {Representation issues in the ML estimation of camera motion},
volume = {1},
isbn = {0-7695-0164-8},
bookTitle = {Proc. of the IEEE Int. Conf. on Computer Vision},
doi = {10.1109/ICCV.1999.791285}
}

@inproceedings{Levinson2013AutomaticOC,
  title={Automatic Online Calibration of Cameras and Lasers},
  author={J. Levinson and S. Thrun},
  booktitle={Robotics: Science and Systems},
  year={2013}
}

@inproceedings{Velas2014CalibrationOR,
  title={Calibration of RGB camera with velodyne LiDAR},
  author={M. Velas and M. Spanel and Z. Materna and A. Herout},
  year={2014}
}

@inproceedings{Geiger2012,
  title={Automatic camera and range sensor calibration using a single shot},
  author={A. Geiger and F. Moosmann and O. Car and B. Schuster},
  booktitle={Proc. IEEE Int. Conf. Robot. Autom.},
  pages={3936-3943},
  year={2012}
}

@article{Guindel2017,
  title={Automatic extrinsic calibration for lidar-stereo vehicle sensor setups},
  author={C. Guindel and J Beltr{\'a}n and D. Mart{\'i}n and F. Garc{\'i}a},
  journal={2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC)},
  year={2017},
  pages={1-6}
}

@InProceedings{Oliveira2019,
author="M. Oliveira and A. Castro and T. Madeira and P. Dias and V. Santos",
title={{A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS}},
booktitle="Robot 2019: Fourth Iberian Robotics Conf.",
year="2020",
publisher="Springer Int. Publishing",
address="Cham",
pages="203--215",
isbn="978-3-030-35990-4"
}

@article{Bradski2000,
    author = {G. Bradski},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@inproceedings{Tsai1988,
    author = { R Tsai and  R. Lenz},
    booktitle = {IEEE Int. Conf. on Robotics and Automation},
    doi = {10.1109/ROBOT.1988.12110},
    isbn = {0-8186-0852-8},
    pages = {554--561},
    publisher = {IEEE Comput. Soc. Press},
    title = {{Real Time Versatile Robotics Hand/Eye Calibration using 3D Machine Vision}},
    year = {1988},
}

@article{Tsai1989,
    author = { R. Tsai and  R. Lenz},
    doi = {10.1109/70.34770},
    issn = {1042296X},
    journal = {IEEE Trans. on Robotics and Automation},
    month = {jun},
    number = {3},
    pages = {345--358},
    title = {{A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/Eye Calibration}},
    volume = {5},
    year = {1989}
}



@article{Koide2019,
    author = {K. Koide and E. Menegatti},
    title = {{General Hand-Eye Calibration Based on Reprojection Error Minimization}},
    keywords = {Calibration and identification,industrial robots},
    issn = {23773766},
    number = {2},
    pages = {1021--1028},
    publisher = {IEEE},
    volume = {4},
    year = {2019},
    journal = {IEEE Robotics and Automation Letters},
    doi = {10.1109/LRA.2019.2893612},
}

@article{Shiu1989,
    author = {Y. C. Shiu and S. Ahmad},
    title = {{Calibration of Wrist-Mounted Robotic Sensors by Solving Homogeneous Transform Equations of the Form AX=XB}},
    journal = {IEEE Trans. on Robotics and Automation},
    issn = {1042296X},
    number = {1},
    pages = {16--29},
    volume = {5},
    year = {1989},
    doi = {10.1109/70.88014},
}

@misc{opencv,
    title={Open Source Computer Vision Library},
    author={OpenCV},
    year={2020},
    howpublished = {\url{https://github.com/opencv/opencv}}
}

@article{Lim2019636,
    author={G. H. Lim and N. Lau and E. Pedrosa and F. Amaral and A. Pereira and L. Azevedo and B. J. and Cunha},
    title={{Precise and Efficient Pose Estimation of Stacked Objects for Mobile Manipulation in Industrial Robotics Challenges}},
    journal={Advanced Robotics},
    publisher = {Taylor \& Francis},
    year={2019},
    volume={33},
    number={13},
    issn={0169-1864},
    pages={636-646},
    doi={10.1080/01691864.2019.1617780},
}

@ARTICLE{Correll2018,
    author={N. {Correll} and K. E. {Bekris} and D. {Berenson} and O. {Brock} and A. {Causo} and K. {Hauser} and K. {Okada} and A. {Rodriguez} and J. M. {Romano} and P. R. {Wurman}},
    journal={IEEE Trans. on Automation Science and Engineering},
    title={{Analysis and Observations From the First Amazon Picking Challenge}},
    year={2018},
    volume={15},
    number={1},
    pages={172-188},
    doi={10.1109/TASE.2016.2600527},
    ISSN={1558-3783},
    month={Jan},
}

@InProceedings{Agarwal2010,
author="S. Agarwal and N. Snavely and S. Seitz and and R. Szeliski",
title="Bundle Adjustment in the Large",
booktitle="Computer Vision - ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="29--42",
isbn="978-3-642-15552-9"
}

@article{Marchand2005,
   Author = {E. Marchand and F. Spindler and F. Chaumette},
   Title = {ViSP for visual servoing: a generic software platform with a wide class of robot control skills},
   Journal = {IEEE Robotics and Automation Magazine},
   Volume = {12},
   Number = {4},
   Pages = {40--52},
   Publisher = {IEEE},
   Month = {December},
   Year = {2005}
}

@InProceedings{Trinh2018,
   Author = {S. Trinh and F. Spindler and E. Marchand and F. Chaumette},
   Title = {A modular framework for model-based visual tracking using edge, texture and depth features},
   BookTitle = {{IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'18}},
   Address = {Madrid, Spain},
   Month = {October},
   Year = {2018}
}

@INPROCEEDINGS{Zhang2017,
  author={Y. Zhang and G. Li and X. Xie and Z. Wang},
  booktitle={2017 IEEE Int. Symposium on Circuits and Systems (ISCAS)},
  title={A new algorithm for accurate and automatic chessboard corner detection},
  year={2017},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ISCAS.2017.8050637}}

@article{Garrido2016,
title = "Generation of fiducial marker dictionaries using Mixed Integer Linear Programming",
journal = "Pattern Recognition",
volume = "51",
pages = "481 - 491",
year = "2016",
issn = "0031-3203",
doi = "10.1016/j.patcog.2015.09.023",
author = "S. Jurado and R. Salinas and F. Cuevas and R. Carnicer",
keywords = "Fiducial markers, MILP, Mixed Integer Linear Programming, Augmented reality, Computer vision"
}

@article{Romero2018,
title = "Speeded up detection of squared fiducial markers",
journal = "Image and Vision Computing",
volume = "76",
pages = "38 - 47",
year = "2018",
issn = "0262-8856",
doi = "10.1016/j.imavis.2018.05.004",
author = "F. Ramirez and F. Salinas and R. Carnicer",
keywords = "Fiducial markers, Marker mapping, SLAM",
}

@INPROCEEDINGS{Hu2019,
  author={D. {Hu} and D. {DeTone} and T. {Malisiewicz}},
  booktitle={2019 IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)},
  title={Deep ChArUco: Dark ChArUco Marker Pose Estimation},
  year={2019},
  volume={},
  number={},
  pages={8428-8436}
}


@article{Gao2003,
  title={Complete Solution Classification for the Perspective-Three-Point Problem},
  author={X. Gao and X. Hou and J. Tang and H. Cheng},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2003},
  volume={25},
  pages={930-943}
}

@ARTICLE{Sanchez2013,
  author={A. {Sanchez} and J. {Cetto} and F. {Noguer}},
  journal={IEEE Trans. on Pattern Analysis and Machine Intelligence},
  title={Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation},
  year={2013},
  volume={35},
  number={10},
  pages={2387-2400}
}

@InProceedings{Fabbri2012,
author="R. Fabbri and B. B Kimia and P. J. Giblin",
title="Camera Pose Estimation Using First-Order Curve Differential Geometry",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="231--244",
isbn="978-3-642-33765-9"
}

@article{Oliveira2020,
title = {{A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach}},
journal = "Robotics and Autonomous Systems",
pages = "103558",
year = "2020",
issn = "0921-8890",
doi = "10.1016/j.robot.2020.103558",
author = "M. Oliveira and A. Castro and T. Madeira and E. Pedrosa and P. Dias and V. Santos",
keywords = "Extrinsic calibration, ROS, Optimization, Bundle adjustment, Intelligent vehicles, OpenCV",
}

@book{Abbaspour2007,
    title={Basic Lie Theory},
    author={H. Abbaspour and M. Moskowitz},
    year={2007},
    publisher={World Scientific Publishing Company},
    doi={10.1142/6462},
    ISBN={9789812790293},
}

@article{Wu2016,
author = {L. Wu and Liao and J. Wang and L. Qi and K. Wu and H. Ren and M. Q. H. Meng},
doi = {10.1109/TRO.2016.2530079},
file = {:home/efp/Dropbox/papers/2016/Wu et al/Wu et al. - 2016 - Simultaneous Hand-Eye, Tool-Flange, and Robot-Robot Calibration for Comanipulation by Solving the AXB = YCZ Problem.pdf:pdf},
issn = {15523098},
journal = {IEEE Trans. on Robotics},
keywords = {AXB = YCZ,calibration,cooperative manipulators,hand-eye calibration,identification},
mendeley-groups = {Hand-Eye Calib},
number = {2},
pages = {413--428},
title = {{Simultaneous Hand-Eye, Tool-Flange, and Robot-Robot Calibration for Comanipulation by Solving the AXB = YCZ Problem}},
volume = {32},
year = {2016}
}

@article{Ren2018,
author = {S. Ren and X. Yang and Y. Song and H. Qiao and L. Wu and J. Xu and K. Chen},
doi = {10.1109/CYBER.2017.8446185},
file = {:home/efp/Dropbox/papers/2018/Ren et al/Ren et al. - 2018 - A Simultaneous Hand-Eye Calibration Method for Hybrid Eye-in-HandEye-to-Hand System.pdf:pdf},
isbn = {9781538604892},
journal = {2017 IEEE 7th Annual Int. Conf. on CYBER Technology in Automation, Control, and Intelligent Systems, CYBER 2017},
keywords = {Hand-eye calibration,Hand-in-eye,Hand-to-eye,Hybrid camera system,Simultaneous calibration method},
mendeley-groups = {Hand-Eye Calib},
pages = {568--573},
title = {{A Simultaneous Hand-Eye Calibration Method for Hybrid Eye-in-Hand/Eye-to-Hand System}},
year = {2018}
}

@article{Coleman1996,
    author = {T. F. Coleman and Y. Li},
    title = {{An Interior Trust Region Approach for Nonlinear Minimization Subject to Bounds}},
    journal = {{SIAM Journal on Optimization}},
    volume = {6},
    number = {2},
    pages = {418-445},
    year = {1996},
    doi = {10.1137/0806023},

}


@ARTICLE{Malti2013-ph,
  title    = "Hand--eye calibration with epipolar constraints: Application to
              endoscopy",
  author   = "A. Malti",
  journal  = "Robotics and Autonomous Systems",
  volume   =  61,
  number   =  2,
  pages    = "161--169",
  month    =  feb,
  year     =  2013,
  keywords = "Hand--eye calibration; Radial distortion; Endoscopy; 3D
              reconstruction",
  issn     = "0921-8890",
  doi      = "10.1016/j.robot.2012.09.029"
}


@INPROCEEDINGS{Ruan2014,  author={M. {Ruan} and D. {Huber}},  
booktitle={2014 2nd Int. Conf. on 3D Vision},   
title={Calibration of 3D Sensors Using a Spherical Target},   year={2014},  volume={1},  number={},  pages={187-193},  doi={10.1109/3DV.2014.100}}

@INPROCEEDINGS{Rato2020,
author={D. {Rato} and V. {Santos}},
booktitle={2020 IEEE Int. Conf. on Autonomous Robot Systems and Competitions (ICARSC)},
title={Automatic Registration of IR and RGB Cameras using a Target detected with Deep Learning},
year={2020},
volume={},
number={},  pages={287-293},
}


@article{Oliveira2015,
title = "Multimodal inverse perspective mapping",
journal = "Information Fusion",
volume = "24",
pages = "108 - 121",
year = "2015",
issn = "1566-2535",
doi = "10.1016/j.inffus.2014.09.003",
author = "M. Oliveira and V. Santos and A. Sappa",
keywords = "Inverse perspective mapping, Multimodal sensor fusion, Intelligent vehicles",
}

@article{Badue2021,
title = "Self-driving cars: A survey",
journal = "Expert Systems with Applications",
volume = "165",
pages = "113816",
year = "2021",
issn = "0957-4174",
doi = "10.1016/j.eswa.2020.113816",
author = "C. Badue and R. Guidolini and R. Carneiro and P. Azevedo and V. Cardoso and A. Forechi and L. Jesus and R. Berriel and T. Paixão and  F. Mutz and L. {Veronese} and T. Santos and A. {De Souza}",
keywords = "Self-driving cars, Robot localization, Occupancy grid mapping, Road mapping, Moving objects detection, Moving objects tracking, Traffic signalization detection, Traffic signalization recognition, Route planning, Behavior selection, Motion planning, Obstacle avoidance, Robot control",
abstract = "We survey research on self-driving cars published in the literature focusing on autonomous cars developed since the DARPA challenges, which are equipped with an autonomy system that can be categorized as SAE level 3 or higher. The architecture of the autonomy system of self-driving cars is typically organized into the perception system and the decision-making system. The perception system is generally divided into many subsystems responsible for tasks such as self-driving-car localization, static obstacles mapping, moving obstacles detection and tracking, road mapping, traffic signalization detection and recognition, among others. The decision-making system is commonly partitioned as well into many subsystems responsible for tasks such as route planning, path planning, behavior selection, motion planning, and control. In this survey, we present the typical architecture of the autonomy system of self-driving cars. We also review research on relevant methods for perception and decision making. Furthermore, we present a detailed description of the architecture of the autonomy system of the self-driving car developed at the Universidade Federal do Espírito Santo (UFES), named Intelligent Autonomous Robotics Automobile (IARA). Finally, we list prominent self-driving car research platforms developed by academia and technology companies, and reported in the media."
}

@article{DePaula2014,
  doi = {10.1016/j.eswa.2013.08.096},
  year = {2014},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {41},
  number = {4},
  pages = {1997--2007},
  author = {M. de Paula and C. Jung and L. Silveira},
  title = {Automatic on-the-fly extrinsic camera calibration of onboard vehicular cameras},
  journal = {Expert Systems with Applications}
}

@article{Alvarez2014,
  doi = {10.1016/j.eswa.2013.08.050},
  year = {2014},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {41},
  number = {4},
  pages = {1532--1542},
  author = {S. {\'{A}}lvarez and D. Llorca and M. Sotelo},
  title = {Hierarchical camera auto-calibration for traffic surveillance systems},
  journal = {Expert Systems with Applications}
}

@Article{Lourenco2020,
author={B. Louren{\c{c}}o and T. Madeira and P. Dias and V. Santos and M. Oliveira},
title={2D lidar to kinematic chain calibration using planar features of indoor scenes},
journal={Industrial Robot: the Int. journal of robotics research and application},
year={2020},
month={Jan},
day={01},
publisher={Emerald Publishing Limited},
volume={47},
number={5},
pages={647-655},
issn={0143-991X},
doi={10.1108/IR-09-2019-0201},
}


@article{Durrant-Whyte2006,
  doi = {10.1109/mra.2006.1638022},
  year = {2006},
  month = jun,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {13},
  number = {2},
  pages = {99--110},
  author = {H. Whyte and T. Bailey},
  title = {Simultaneous localization and mapping: part I},
  journal = {{IEEE} Robotics {\&} Automation Magazine}
}

@inproceedings{pathplanning,
  doi = {10.1109/scored.2006.4339335},
  year = {2006},
  month = jun,
  publisher = {{IEEE}},
  author = {N. Sariff and N. Buniyamin},
  title = {An Overview of Autonomous Mobile Robot Path Planning Algorithms},
  booktitle = {2006 4th Student Conf. on Research and Development}
}

@article{dosSantos2016,
  doi = {10.1007/s10846-016-0340-5},
  year = {2016},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {83},
  number = {3-4},
  pages = {429--444},
  author = {P. Neves dos Santos and H. Sobreira and D. Campos and R. Morais and A. P. Moreira and O. Contente},
  title = {Towards a Reliable Robot for Steep Slope Vineyards Monitoring},
  journal = {Journal of Intelligent {\&} Robotic Systems}
}

@inproceedings{Huang2009,
  doi = {10.1109/ivs.2009.5164263},
  year = {2009},
  month = jun,
  publisher = {{IEEE}},
  author = {L. Huang and M. Barth},
  title = {A novel multi-planar {LiDAR} and computer vision calibration procedure using 2D patterns for automated navigation},
  booktitle = {2009 {IEEE} Intelligent Vehicles Symposium}
}

@INPROCEEDINGS{1904.12433,
  title     = "Automatic extrinsic calibration between a camera and a {3D}
               Lidar using {3D} point and plane correspondences",
  booktitle = "2019 {IEEE} Intelligent Transportation Systems Conf.
               ({ITSC})",
  author    = "S. Verma and J. Berrio and S. Worrall and E. Nebot",
  publisher = "IEEE",
  pages     = "3906--3912",
  month     =  "oct",
  year      =  2019,
  address   =  "Auckland, New Zealand",
  doi       = "10.1109/ITSC.2019.8917108"
}

@article{Wang2017,
  doi = {10.3390/rs9080851},
  year = {2017},
  month = aug,
  publisher = {{MDPI} {AG}},
  volume = {9},
  number = {8},
  pages = {851},
  author = {W. Wang and K. Sakurada and N. Kawaguchi},
  title = {Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D {LiDAR} and Panoramic Camera Using a Printed Chessboard},
  journal = {Remote Sensing}
}

@inproceedings{Fremont2008,
  doi = {10.1109/mfi.2008.4648067},
  year = {2008},
  month = aug,
  publisher = {{IEEE}},
  author = {V. Fremont and P. Bonnifait},
  title = {Extrinsic calibration between a multi-layer lidar and a camera},
  booktitle = {2008 {IEEE} Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems}
}


@article{Mirzaei2012,
  doi = {10.1177/0278364911435689},
  year = {2012},
  month = apr,
  publisher = {{SAGE} Publications},
  volume = {31},
  number = {4},
  pages = {452--467},
  author = {F. M. Mirzaei and D. G. Kottas and S. I. Roumeliotis},
  title = {3D {LiDAR}{\textendash}camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization},
  journal = {The Int. Journal of Robotics Research}
}

@article{Pandey2010,
  doi = {10.3182/20100906-3-it-2019.00059},
  year = {2010},
  publisher = {Elsevier {BV}},
  volume = {43},
  number = {16},
  pages = {336--341},
  author = {G. Pandey and J. McBride and S. Savarese and R. Eustice},
  title = {Extrinsic Calibration of a 3D Laser Scanner and an Omnidirectional Camera},
  journal = {{IFAC} Proceedings Volumes}
}

@inproceedings{Zhou2018,
  doi = {10.1109/iros.2018.8593660},
  year = {2018},
  month = oct,
  publisher = {{IEEE}},
  author = {L. Zhou and Z. Li and M. Kaess},
  title = {Automatic Extrinsic Calibration of a Camera and a 3D {LiDAR} Using Line and Plane Correspondences},
  booktitle = {2018 {IEEE}/{RSJ} Int. Conf. on Intelligent Robots and Systems ({IROS})}
}

@article{Huang2020,
  doi = {10.1109/access.2020.3010734},
  year = {2020},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {8},
  pages = {134101--134110},
  author = {J. Huang and J. Grizzle},
  title = {Improvements to Target-Based 3D {LiDAR} to Camera Calibration},
  journal = {{IEEE} Access}
}

@misc{Dhall2017,
Author = {A. Dhall and K. Chelani and V. Radhakrishnan and K. M. Krishna},
Title = {LiDAR-Camera Calibration using 3D-3D Point correspondences},
Year = {2017},
Eprint = {arXiv:1705.09785},
}

@article{Kim2019,
  doi = {10.3390/s20010052},
  year = {2019},
  month = dec,
  publisher = {{MDPI} {AG}},
  volume = {20},
  number = {1},
  pages = {52},
  author = {E. Kim and S. Park},
  title = {Extrinsic Calibration between Camera and {LiDAR} Sensors by Matching Multiple 3D Planes},
  journal = {Sensors}
}

@article{ZunigaNoel2019,
  doi = {10.1109/lra.2019.2922618},
  year = {2019},
  month = jul,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {4},
  number = {3},
  pages = {2862--2869},
  author = {D. Noel and J. Sarmiento and R. Ojeda and J. Jimenez},
  title = {Automatic Multi-Sensor Extrinsic Calibration For Mobile Robots},
  journal = {{IEEE} Robotics and Automation Letters}
}



@InProceedings{bundleadjustment,
author="S. Agarwal and 
N. Snavely and
S. M. Seitz and
R. Szeliski",
title="Bundle Adjustment in the Large",
booktitle="Computer Vision -- ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="29--42",
abstract="We present the design and implementation of a new inexact Newton type algorithm for solving large-scale bundle adjustment problems with tens of thousands of images. We explore the use of Conjugate Gradients for calculating the Newton step and its performance as a function of some simple and computationally efficient preconditioners. We show that the common Schur complement trick is not limited to factorization-based methods and that it can be interpreted as a form of preconditioning. Using photos from a street-side dataset and several community photo collections, we generate a variety of bundle adjustment problems and use them to evaluate the performance of six different bundle adjustment algorithms. Our experiments show that truncated Newton methods, when paired with relatively simple preconditioners, offer state of the art performance for large-scale bundle adjustment. The code, test problems and detailed performance data are available at http://grail.cs.washington.edu/projects/bal.",
isbn="978-3-642-15552-9"
}

@inproceedings{ours1,
  doi = {10.1109/itsc.2017.8317829},
  year = {2017},
  month = oct,
  publisher = {{IEEE}},
  author = {C. Guindel and J. Beltran and D. Martin and F. Garcia},
  title = {Automatic extrinsic calibration for lidar-stereo vehicle sensor setups},
  booktitle = {2017 {IEEE} 20th Int. Conf. on Intelligent Transportation Systems ({ITSC})}
}

@InProceedings{ours2,
author="M. Oliveira and A. Castro and T. Madeira and P. Dias and V. Santos",
editor="Silva, Manuel F.
and Lu{\'i}s Lima, Jos{\'e}
and Reis, Lu{\'i}s Paulo
and Sanfeliu, Alberto
and Tardioli, Danilo",
title="A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS",
booktitle="Robot 2019: Fourth Iberian Robotics Conf.",
year="2020",
publisher="Springer Int. Publishing",
address="Cham",
pages="203--215",
abstract="Intelligent vehicles are complex systems which often accommodate several sensors of different modalities. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration process. The calibration problem is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations.",
isbn="978-3-030-35990-4"
}

@article{ours3,
  doi = {10.1016/j.robot.2020.103558},
  year = {2020},
  month = sep,
  publisher = {Elsevier {BV}},
  volume = {131},
  pages = {103558},
  author = {M. Oliveira and A. Castro and T. Madeira and E. Pedrosa and P. Dias and V. Santos},
  title = {A {ROS} framework for the extrinsic calibration of intelligent vehicles: A multi-sensor,  multi-modal approach},
  journal = {Robotics and Autonomous Systems}
}

@article{agrob1,
  doi = {10.1016/j.compag.2020.105535},
  year = {2020},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {175},
  pages = {105535},
  author = {A. Aguiar and F. Santos and L. C. Santos and V. Filipe and A. Sousa},
  title = {Vineyard trunk detection using deep learning {\textendash} An experimental device benchmark},
  journal = {Computers and Electronics in Agriculture}
}

@article{agrob2,
  doi = {10.1017/s0263574719000961},
  year = {2019},
  month = jul,
  publisher = {Cambridge University Press ({CUP})},
  volume = {38},
  number = {4},
  pages = {684--698},
  author = {L. Santos and F. Santos and J. Mendes and P Costa and J Lima and R. Reis and P. Shinde},
  title = {Path Planning Aware of Robot's Center of Mass for Steep Slope Vineyards},
  journal = {Robotica}
}

@inproceedings{ROS,
  title={ROS: an open-source Robot Operating System},
  author={M. Quigley and K. Conley and B. Gerkey and J. Faust and T. Foote and J. Leibs and R. Wheeler and A. Y. Ng},
  booktitle={ICRA workshop on open source software},
  volume={3},
  number={3.2},
  pages={5},
  year={2009},
  organization={Kobe, Japan}
}

@inproceedings{axisangle,
  doi = {10.1109/iccv.1999.791285},
  year = {1999},
  publisher = {{IEEE}},
  author = {J. Hornegger and C. Tomasi},
  title = {Representation issues in the {ML} estimation of camera motion},
  booktitle = {Proceedings of the Seventh {IEEE} Int. Conf. on Computer Vision}
}

@inproceedings{tftree,
  doi = {10.1109/tepra.2013.6556373},
  year = {2013},
  month = apr,
  publisher = {{IEEE}},
  author = {T. Foote},
  title = {tf: The transform library},
  booktitle = {2013 {IEEE} Conf. on Technologies for Practical Robot Applications ({TePRA})}
}

@article{pnp1,
  doi = {10.1109/tpami.2003.1217599},
  year = {2003},
  month = aug,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {25},
  number = {8},
  pages = {930--943},
  author = {X. S. Gao and X. R. Hou and J. Tang and H. F. Cheng},
  title = {Complete solution classification for the perspective-three-point problem},
  journal = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence}
}

@article{pnp2,
  doi = {10.1109/tpami.2020.2985310},
  year = {2020},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--1},
  author = {R. Fabbri and P. Giblin and B. Kimia},
  title = {Camera Pose Estimation Using First-Order Curve Differential Geometry},
  journal = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence}
}

@article{pnp3,
  doi = {10.1109/tpami.2013.36},
  year = {2013},
  month = oct,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {35},
  number = {10},
  pages = {2387--2400},
  author = {A. Penate-Sanchez and J. Andrade-Cetto and F. Moreno-Noguer},
  title = {Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation},
  journal = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence}
}





@article{ransac,
  doi = {10.1145/358669.358692},
  year = {1981},
  month = jun,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {24},
  number = {6},
  pages = {381--395},
  author = {M. A. Fischler and R. C. Bolles},
  title = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  journal = {Communications of the {ACM}}
}



@article{ICP,
  doi = {10.1109/34.121791},
  year = {1992},
  month = feb,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {14},
  number = {2},
  pages = {239--256},
  author = {P.J. Besl and Neil D. McKay},
  title = {A method for registration of 3-D shapes},
  journal = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence}
}



@Inbook{pinholecm,
author="P. Sturm",
editor="Ikeuchi, Katsushi",
title="Pinhole Camera Model",
bookTitle="Computer Vision: A Reference Guide",
year="2014",
publisher="Springer US",
address="Boston, MA",
pages="610--613",
isbn="978-0-387-31439-6",
doi="10.1007/978-0-387-31439-6_472",
}

@article{datafusion1,
title = "A data fusion system of GNSS data and on-vehicle sensors data for improving car positioning precision in urban environments",
journal = "Expert Systems with Applications",
volume = "80",
pages = "28 - 38",
year = "2017",
issn = "0957-4174",
doi = "10.1016/j.eswa.2017.03.018",
author = "C. Melendez-Pastor and R. Ruiz-Gonzalez and J. Gomez-Gil",
keywords = "Data fusion, Extended Kalman filter (EKF), Global navigation satellite system (GNSS), On-vehicle sensors, On-board diagnostics (OBD), Ackerman steering geometry",
}

@article{datafusion2,
title = "Multi-sensors data fusion through fuzzy clustering and predictive tools",
journal = "Expert Systems with Applications",
volume = "107",
pages = "165 - 172",
year = "2018",
issn = "0957-4174",
doi = "10.1016/j.eswa.2018.04.026",
author = "S. Majumder and D. K. Pratihar",
keywords = "Multi-sensors data, Input-output relationships, Fuzzy clustering, Fuzzy reasoning tool",
abstract = "Sensory data are generally associated with imprecision and uncertainty, and consequently, it becomes difficult to extract useful information from them. The problem becomes even more difficult to handle, when the data are collected using multiple sensors. Realizing the ability of fuzzy sets to deal with imprecision and uncertainty, a multi-sensors data fusion technique was developed in this study by using fuzzy clustering and predictive tools. The data were first clustered based on their similarity using an entropy-based fuzzy C-means clustering technique and the obtained clusters were utilized to develop a fuzzy reasoning-based predictive tool. The novelty of this study lies with the application of a clustering algorithm, which can ensure both compactness and distinctness of the developed clusters, and development of a reasoning tool utilizing the information of obtained clusters. Two types of multi-sensors data were used to test the performance of the proposed algorithm. Results were compared with those available in the literature, and the developed technique was found to perform better than the previous approaches on both the data sets. The better performance of the proposed algorithm could be due to its in-depth search of the data set through similarity-based fuzzy clustering followed by the development of fuzzy reasoning tool utilizing the information of obtained clusters."
}

@article{selfdriving,
title = "Self-driving cars: A survey",
journal = "Expert Systems with Applications",
volume = "165",
pages = "113816",
year = "2021",
issn = "0957-4174",
doi = "10.1016/j.eswa.2020.113816",
author = "C. Badue and R. Guidolini R. Vivacqua Carneiro and P. Azevedo and V. B. Cardoso and A. Forechi and L. Jesus and R. Berriel and T. M. Paixão and F. Mutz and L. {de Paula Veronese} and T. Oliveira-Santos and A. F. {De Souza}",
keywords = "Self-driving cars, Robot localization, Occupancy grid mapping, Road mapping, Moving objects detection, Moving objects tracking, Traffic signalization detection, Traffic signalization recognition, Route planning, Behavior selection, Motion planning, Obstacle avoidance, Robot control",
abstract = "We survey research on self-driving cars published in the literature focusing on autonomous cars developed since the DARPA challenges, which are equipped with an autonomy system that can be categorized as SAE level 3 or higher. The architecture of the autonomy system of self-driving cars is typically organized into the perception system and the decision-making system. The perception system is generally divided into many subsystems responsible for tasks such as self-driving-car localization, static obstacles mapping, moving obstacles detection and tracking, road mapping, traffic signalization detection and recognition, among others. The decision-making system is commonly partitioned as well into many subsystems responsible for tasks such as route planning, path planning, behavior selection, motion planning, and control. In this survey, we present the typical architecture of the autonomy system of self-driving cars. We also review research on relevant methods for perception and decision making. Furthermore, we present a detailed description of the architecture of the autonomy system of the self-driving car developed at the Universidade Federal do Espírito Santo (UFES), named Intelligent Autonomous Robotics Automobile (IARA). Finally, we list prominent self-driving car research platforms developed by academia and technology companies, and reported in the media."
}

@article{Pinto2020,
title = "MARESye: A hybrid imaging system for underwater robotic applications",
journal = "Information Fusion",
volume = "55",
pages = "16 - 29",
year = "2020",
issn = "1566-2535",
doi = "10.1016/j.inffus.2019.07.014",
author = "A. Pinto and A. Matos",
keywords = "Underwater imaging, Image fusion, Active imaging, Passive imaging, Underwater, Robotics",
}

@article{Jiuqing2018,
title = "Distributed data association in smart camera network via dual decomposition",
journal = "Information Fusion",
volume = "39",
pages = "120 - 138",
year = "2018",
issn = "1566-2535",
doi = "10.1016/j.inffus.2017.04.007",
author = "W. Jiuqing and C. Xu and B. Shaocong and L. Li",
keywords = "Data association, Smart camera network, Dual decomposition, Distributed algorithm",
abstract = "One of the fundamental requirements for pedestrian surveillance using smart camera network is the correct association of each person's observations generated on different camera nodes to the person's track. Recently, distributed data association methods that involve only local information processing on each camera node and mutual information exchanging between neighboring cameras have attracted many research interests due to their superiority in large scale applications. In this paper, we propose a new method that performs global data association in a distributed manner by fusing the appearance and spatio-temporal measurements of objects captured by all camera nodes in the entire network. Specifically, we formulate the data association problem in smart camera networks as an Integer Programming problem by introducing a set of linking variables, and propose two distributed algorithms, namely L-DD and Q-DD, to solve the Integer Programming problem using the dual decomposition technique. In our algorithms, the original Integer Programming problem is decomposed into several subproblems, which can be solved locally on each smart camera. Different subproblems reach consensus on their solutions in a rigorous way by adjusting their parameters iteratively based on the projected subgradient optimization. The proposed method is simple and flexible, in that (i) we can incorporate any feature extraction and matching technique into our framework to calculate the similarity between observations, which corresponds to the costs of links in our model, and (ii) we can decompose the original problem in any way as long as the resulting subproblem can be solved independently and efficiently. We show the competitiveness of our method in both accuracy and speed by theoretical analysis and experimental comparison with state-of-the-art algorithms."
}

@article{Hanning2011,
title = "Calibration and low-level data fusion algorithms for a parallel 2D/3D-camera",
journal = "Information Fusion",
volume = "12",
number = "1",
pages = "37 - 47",
year = "2011",
note = "Special Issue on Intelligent Transportation Systems",
issn = "1566-2535",
doi = "10.1016/j.inffus.2010.01.006",
author = "T. Hanning and A. Lasaruk and T. Tatschke",
keywords = "Fusion of range and image data, Calibration, Nearest neighbor, Nearest neighbor by segmentation",
abstract = "In this article we propose a calibration algorithm and three low-level data fusion algorithms for a parallel 2D/3D-camera system. A parallel 2D/3D-camera is a hardware setup of a range camera and a high-resolution gray-value camera spatially related to each other by a fixed translation. The proposed calibration algorithm utilizes the fact that for known calibration patterns the range reconstruction accuracy of the gray-value camera is significantly higher than that of the range sensor. Using the calibrated 2D/3D-camera we identify the range pixels within the gray-value image for each pair of acquired camera images. We present three low-level data fusion approaches assigning range information to each gray-value pixel based on different neighborhood relations to the identified range pixels: one-nearest neighbor, nearest neighbors in the surrounding Delaunay-triangle, and nearest neighbors constrained by a gray-value image segmentation. We demonstrate the applicability, efficiency and accuracy of our calibration and fusion algorithms on real and synthetic data. Our real experiments are performed on a 2D/3D-camera comprising a Siemens 64×8-pixel time-of-flight range camera developed within the European project PReVENT (UseRCams) and a common gray-value camera."
}

@article{Boucher2001,
title = "3D structure and motion recovery in a multisensor framework",
journal = "Information Fusion",
volume = "2",
number = "4",
pages = "271 - 285",
year = "2001",
issn = "1566-2535",
doi = "10.1016/S1566-2535(01)00045-8",
author = "C. Boucher and J. Noyer and M. Benjelloun",
keywords = "Sensor fusion, Non-linear filtering, Computer vision, Structure and motion analysis",
abstract = "The aim of this article is to develop a multisensor estimation method to identify the 3D structure and motion of an object. The method lies in the feature description of the object and the solution uses an extended Kalman filter (EKF) which fuses information from each sensor. The filter tracks the features through the data sequences and estimates the 3D position and affines motion parameters. The originality of this work relies on a 3D modelling of this problem to jointly estimate the 3D structure and motion. This estimation is made possible by the use of an active sensor (range camera)."
}

@article{Rasto2020,
title = "Remote sensing image classification using subspace sensor fusion",
journal = "Information Fusion",
volume = "64",
pages = "121 - 130",
year = "2020",
issn = "1566-2535",
doi = "10.1016/j.inffus.2020.07.002",
author = "B. Rasti and P. Ghamisi",
keywords = "Multisensor data fusion, Classification, Dimensionality reduction, Feature extraction, Subspace fusion, Remote sensing",
abstract = "The amount of remote sensing and ancillary datasets captured by diverse airborne and spaceborne sensors has been tremendously increased, which opens up the possibility of utilizing multimodal datasets to improve the performance of processing approaches with respect to the application at hand. However, developing a generic framework with high generalization capability that can effectively fuse diverse datasets is a challenging task since the current approaches are usually only applicable to two specific sensors for data fusion. In this paper, we propose an accurate fusion-based technique called SubFus with capability to integrate diverse remote sensing data for land cover classification. Here, we assume that a high dimensional multisensor dataset can be represented fused features that live in a lower-dimensional space. The proposed classification methodology includes three main stages. First, spatial information is extracted by using spatial filters (i.e., morphology filters). Then, a novel low-rank minimization problem is proposed to represent the multisensor datasets in subspaces using fused features. The fused features in the lower-dimensional subspace are estimated using a novel iterative algorithm based on the alternative direction method of multipliers. Third, the final classification map is produced by applying a supervised spectral classifier (i.e., random forest) on the fused features. In the experiments, the proposed method is applied to a three-sensor (RGB, multispectral LiDAR, and hyperspectral images) dataset captured over the area of the University of Houston, the USA, and a two-sensor (hyperspectral and LiDAR) dataset captured over the city of Trento, Italy. The land-cover maps generated using SubFus are evaluated based on classification accuracies. Experimental results obtained by SubFus confirm considerable improvements in terms of classification accuracies compared with the other methods used in the experiments. The proposed fusion approach obtains 85.32% and 99.25% in terms of overall classification accuracy on the Houston (the training portion of the dataset distributed for the data fusion contest of 2018) and trento datasets, respectively."
}

@article{Huang2010,
author = {A. Huang and M. Antone and E. Olson and L. Fletcher and D. Moore and S. Teller and J. Leonard},
title ={A High-rate, Heterogeneous Data Set From The DARPA Urban Challenge},
journal = {The Int. Journal of Robotics Research},
volume = {29},
number = {13},
pages = {1595-1601},
year = {2010},
doi = {10.1177/0278364910384295},
abstract = { This paper describes a data set collected by MIT’s autonomous vehicle Talos during the 2007 DARPA Urban Challenge. Data from a high-precision navigation system, five cameras, 12 SICK planar laser range scanners, and a Velodyne high-density laser range scanner were synchronized and logged to disk for 90 km of travel. In addition to documenting a number of large loop closures useful for developing mapping and localization algorithms, this data set also records the first robotic traffic jam and two autonomous vehicle collisions. It is our hope that this data set will be useful to the autonomous vehicle community, especially those developing robotic perception capabilities.}
}

@INPROCEEDINGS{Kerl2013,
  AUTHOR    = {C. Kerl and J. Sturm and D. Cremers},
  TITLE     = {Dense Visual SLAM for RGB-D Cameras},
  BOOKTITLE = {Proc. of the Int. Conf. on Intelligent Robot Systems (IROS)},
  YEAR      = {2013},
  KEYWORDS  = {dense visual odometry,dense visual slam,rgb-d,rgb-d benchmark}
}

@inproceedings{Fankhauser2015,
  author = {P. Fankhauser and M. Bloesch and D. Rodriguez and R. Kaestner and M. Hutter and R. Siegwart},
  title = {Kinect v2 for Mobile Robot Navigation: Evaluation and Modeling},
  booktitle = {IEEE Int. Conf. on Advanced Robotics (ICAR)},
  year = {2015}
}

@Article{Santos2020,
AUTHOR = {V. Santos and D. Rato and P. Dias and M. Oliveira},
TITLE = {Multi-Sensor Extrinsic Calibration Using an Extended Set of Pairwise Geometric Transformations},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6717},
ISSN = {1424-8220},
DOI = {10.3390/s20236717}
}


@Article{Aguiar2021,
title = {A Camera to LiDAR calibration approach through the Optimization of Atomic Transformations},
journal = {Expert Systems with Applications},
pages = {114894},
year = {2021},
issn = {0957-4174},
doi = {10.1016/j.eswa.2021.114894},
author = {A. Aguiar and M. Oliveira and E. Pedrosa and F. Santos},
note={IF 5.423 (2020)}
}

@ARTICLE{Pedrosa2021,
  author={E. {Pedrosa} and M. Oliveira and N. {Lau} and V. {Santos}},
  journal={IEEE Trans. on Robotics},
  title={A General Approach to Hand–Eye Calibration Through the Optimization of Atomic Transformations},
  year={2021},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/TRO.2021.3062306},
note={IF 6.123 (2020)}
}

@ARTICLE{Branch1999,
  title     = "A Subspace, Interior, and Conjugate Gradient Method for
               {Large-Scale} {Bound-Constrained} Minimization Problems",
  author    = "M. Branch and T. Coleman and Y. Li",
  abstract  = "A subspace adaptation of the Coleman--Li trust region and
               interior method is proposed for solving large-scale
               bound-constrained minimization problems. This method can be
               implemented with either sparse Cholesky factorization or
               conjugate gradient computation. Under reasonable conditions the
               convergence properties of this subspace trust region method are
               as strong as those of its full-space version.Computational
               performance on various large test problems is reported;
               advantages of our approach are demonstrated. Our experience
               indicates that our proposed method represents an efficient way
               to solve large bound-constrained minimization problems.",
  journal   = "SIAM J. Sci. Comput.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  21,
  number    =  1,
  pages     = "1--23",
  month     =  jan,
  year      =  1999,
  issn      = "1064-8275",
  doi       = "10.1137/S1064827595289108"
}

@misc{Wiedemeyer2014,
  author = {T. Wiedemeyer},
  title = {{IAI Kinect2}},
  organization = {Institute for Artificial Intelligence},
  address = {University Bremen},
  year = {2014 -- 2015},
  howpublished = {\url{https://github.com/code-iai/iai\_kinect2}},
  note = {Accessed June 12, 2015}
}

@INPROCEEDINGS{Kodagoda2006,
  author={K. Kodagoda and A. Alempijevic and J. Underwood and S. Kumar and G. Dissanayake},
  booktitle={2006 9th Int. Conf. on Control, Automation, Robotics and Vision},
  title={Sensor Registration and Calibration using Moving Targets},
  year={2006},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICARCV.2006.345361}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Parkinson2012,
title = {Automatic planning for machine tool calibration: A case study},
journal = {Expert Systems with Applications},
volume = {39},
number = {13},
pages = {11367-11377},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.03.054},
author = {S. Parkinson and A.P. Longstaff and S. Fletcher and A. Crampton and P. Gregory},
keywords = {Machine tool calibration, Pseudo-static geometric errors, Planning, HTN, PDDL},
abstract = {Machine tool owners require knowledge of their machine’s capabilities, and the emphasis increases with areas of high accuracy manufacturing. An aspect of a machine’s capability is its geometric accuracy. International Standards and best-practice guides are available to aid understanding of the required measurements and to advise on how to perform them. However, there is an absence of any intelligent method capable of optimising the duration of a calibration plan, minimising machine down-time. In this work, artificial intelligence in the form of automated planning is applied to the problem of machine tool pseudo-static geometric error calibration. No prior knowledge of Artificial Intelligence (AI) planning is required throughout this paper. The authors have written this paper for calibration engineers to see the benefits that automated planning can provide. Two models are proposed; the first produces a sequential calibration plan capable of finding the optimal calibration plan. The second model has the additional possibility of planning for concurrent measurements, adding the possibility of further reducing machine down-time. Both models take input regarding a machine’s configuration and available instrumentation. The efficacy of both models is evaluated by performing a case study of a five-axis gantry machine, whereby calibration plans are produced and compared against both an academic and industrial expert. From this, the effectiveness of this novel method for producing optimal calibration plan is evaluated, stimulating potential for future work.}
}


@article{Mutz2016,
title = {Large-scale mapping in complex field scenarios using an autonomous car},
journal = {Expert Systems with Applications},
volume = {46},
pages = {439-462},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.10.045},
author = {Filipe Mutz and Lucas P. Veronese and Thiago Oliveira-Santos and Edilson {de Aguiar} and Fernando A. {Auat Cheein} and Alberto {Ferreira De Souza}},
keywords = {Robotics, Autonomous vehicles, SLAM, GraphSLAM, Mapping},
abstract = {In this paper, we present an end-to-end framework for precise large-scale mapping with applications in autonomous driving. In special, the problem of mapping complex environments, with features changing from tree-lined streets to urban areas with dense traffic, is studied. The robotic car is equipped with an odometry sensor, a 3D LiDAR Velodyne HDL-32E, a IMU, and a low cost GPS, and the data generated by these sensors are integrated in a pose-based GraphSLAM estimator. A new strategy for identification and correction of odometry data using evolutionary algorithms is presented. This new strategy makes odometry data significantly more consistent with GPS. Loop closures are detected using GPS data, and GICP, a 3D point cloud registration algorithm, is used to estimate the displacement between the different travels over the same region. After path estimation, 3D LiDAR data is used to build an occupancy grid mapping of the environment. A detailed mathematical description of how occupancy evidence can be calculated from the point clouds is given, and a submapping strategy to handle memory limitations is presented as well. The proposed framework is tested in three real world environments with different sizes, and features: a parking lot, a university beltway, and a city neighborhood. In all cases, satisfactory maps were built, with precise loop closures even when the vehicle traveled long distances between them.}
}

@article{Jeong2018,
title = {Multimodal sensor-based semantic 3D mapping for a large-scale environment},
journal = {Expert Systems with Applications},
volume = {105},
pages = {1-10},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.03.051},
author = {Jongmin Jeong and Tae Sung Yoon and Jin Bae Park},
keywords = {Semantic mapping, Semantic reconstruction, 3D mapping, Semantic segmentation, 3D refinement},
abstract = {Semantic 3D mapping is one of the most important fields in robotics, and has been used in many applications, such as robot navigation, surveillance, and virtual reality. In general, semantic 3D mapping is mainly composed of 3D reconstruction and semantic segmentation. As these technologies evolve, there has been great progress in semantic 3D mapping in recent years. Furthermore, the number of robotic applications requiring semantic information in 3D mapping to perform high-level tasks has increased, and many studies on semantic 3D mapping have been published. Existing methods use a camera for both 3D reconstruction and semantic segmentation. However, this is not suitable for large-scale environments and has the disadvantage of high computational complexity. To address this problem, we propose a multimodal sensor-based semantic 3D mapping system using a 3D Lidar combined with a camera. In this study, the odometry is obtained by high-precision global positioning system (GPS) and inertial measurement unit (IMU), and it is estimated by iterative closest point (ICP) when a GPS signal is weak. Then, we use the latest 2D convolutional neural network (CNN) for semantic segmentation. To build a semantic 3D map, we integrate the 3D map with semantic information by using coordinate transformation and Bayes’ update scheme. In order to improve the semantic 3D map, we propose a 3D refinement process to correct wrongly segmented voxels and remove traces of moving vehicles in the 3D map. Through experiments on challenging sequences, we demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and intersection over union (IoU). Thus, our method can be used for various applications that require semantic information in 3D map.}
}

@article{Kim2021,
title = {Deep learning-based dynamic object classification using LiDAR point cloud augmented by layer-based accumulation for intelligent vehicles},
journal = {Expert Systems with Applications},
volume = {167},
pages = {113861},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113861},
author = {Kyungpyo Kim and Chansoo Kim and Chulhoon Jang and Myoungho Sunwoo and Kichun Jo},
keywords = {LiDAR, Registration, Deep learning, Dynamic object classification, Intelligent vehicle},
abstract = {An intelligent vehicle must identify the exact position and class of the surrounding object in various situations to consider the interaction with them. For this reason, the light detection and range sensor, called LiDAR, is widely used in intelligent vehicles. The LiDAR provides information in the form of a point cloud that can be used to localize and classify the surrounding objects. However, unlike vision-based object detection and classification system, the LiDAR-based recognition system cannot provide sufficient classification performance even with deep learning technologies. The reason is that the LiDAR point cloud does not have enough shape information to classify the dynamic object due to the sparsity of the points. To address this problem, we proposed a framework to enhance the deep learning-based classification performance by augmenting the shape information of the LiDAR point cloud. The augmented shape information not only improves classification performance of the networks, but also allows deep learning networks to train effectively by using artificial data-set which is generated with 3D computer-aided design model without tedious efforts of labeling. In order to enhance this shape information effectively, also, this paper proposes a layer-based accumulation algorithm considering the three degree-of-freedom motion of a dynamic object. In the experimental results, the proposed accumulation method outperformed existing registration-based methods. In real-vehicle data test, moreover, the deep learning networks trained with artificial data showed better performance when the LiDAR point cloud was accumulated.}
}

@article{Yuan2021,
title = {A review of deep learning methods for semantic segmentation of remote sensing imagery},
journal = {Expert Systems with Applications},
volume = {169},
pages = {114417},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114417},
author = {Xiaohui Yuan and Jianfang Shi and Lichuan Gu},
keywords = {Semantic image segmentation, Deep neural networks, Remote sensing imagery},
abstract = {Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non-conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.}
}

@article{Deng2021,
title = {Multi-obstacle path planning and optimization for mobile robot},
journal = {Expert Systems with Applications},
volume = {183},
pages = {115445},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115445},
author = {Xin Deng and Ruifeng Li and Lijun Zhao and Ke Wang and Xichun Gui},
keywords = {Path planning, Multi-objective optimization, Obstacles-based algorithm, Bezier curve},
abstract = {In the past few decades, many results have been achieved in the research of mobile robot path planning, and they have been applied in simple scenarios, such as factory AGV, bank guide robot. However, path planning in highly dense and complex scenarios has become an important challenge for applications. Robots face dense map and complex obstacles and hardly find out an optimal path within a reasonable period, such as unmanned vehicles in freight ports and rescue robots in earthquake environment. Therefore, a multi-obstacle path planning and optimization method is proposed. In order to simplify complex environmental obstacles, the obstacles will be divided into basis obstacles and extension obstacles. Firstly, the basis obstacles and their contour point sets are determined according to the starting point and goal point. Furthermore, the basis obstacles are optimized by convex hulls, and then the corresponding basis point set is obtained. Secondly, the extension obstacles are determined by the basis point set, starting point and goal point, and then the corresponding extension point set is generated. After that, a path planner is designed by the multi-objective D* Lite algorithm for distance and smoothness in order to get reasonable and optimized path in a complex environment. Moreover, the path is smoothed by cubic bezier curves to fit the kinematic model of the robot. Finally, The proposed method conduct comparative experiments with other algorithms to verify its accuracy and computational efficiency of planning in complex environments.}
}

@article{Li2021,
title = {Creating navigation map in semi-open scenarios for intelligent vehicle localization using multi-sensor fusion},
journal = {Expert Systems with Applications},
volume = {184},
pages = {115543},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115543},
author = {Yicheng Li and Yingfeng Cai and Reza Malekian and Hai Wang and Miguel Angel Sotelo and Zhixiong Li},
keywords = {Intelligent vehicles, Road scenario fingerprint, Multi-view representation, Multi-sensor fusion, Map-based localization}
}

@article{Calderoni2014,
title = {Deploying a network of smart cameras for traffic monitoring on a “city kernel”},
journal = {Expert Systems with Applications},
volume = {41},
number = {2},
pages = {502-507},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.07.076},
author = {Luca Calderoni and Dario Maio and Stefano Rovis},
keywords = {Smart city, Traffic monitoring, Sensors networks, Power line communication, City kernel, Urban innovation},
abstract = {A relevant aspect when evaluating the city smartness is related to the innovative approach to urban traffic management. We present a system called city kernel, designed to handle several subsystems, each addressing a specific sensor network and we describe an infrastructure for wide traffic control via a vision sensor network. This infrastructure consists of a network of smart cameras operating over an outdoor public lighting thanks to power line communication technology and equipped with a vehicle counting and classification algorithm. We discuss the deployment of this network on the city kernel and some related services exposed to urban actors.}
}


@Article{Sappa2016,
AUTHOR = {Sappa, Angel D. and Carvajal, Juan A. and Aguilera, Cristhian A. and Oliveira, Miguel and Romero, Dennis and Vintimilla, Boris X.},
TITLE = {Wavelet-Based Visible and Infrared Image Fusion: A Comparative Study},
JOURNAL = {Sensors},
VOLUME = {16},
YEAR = {2016},
NUMBER = {6},
ARTICLE-NUMBER = {861},
URL = {https://www.mdpi.com/1424-8220/16/6/861},
ISSN = {1424-8220},
DOI = {10.3390/s16060861}
}


@INPROCEEDINGS{Furgale2013,
  author={Furgale, Paul and Rehder, Joern and Siegwart, Roland},
  booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title={Unified temporal and spatial calibration for multi-sensor systems},
  year={2013},
  volume={},
  number={},
  pages={1280-1286},
  doi={10.1109/IROS.2013.6696514}}


@article{OLIVEIRA2022,
title = {ATOM: A general calibration framework for multi-modal, multi-sensor systems},
journal = {Expert Systems with Applications},
pages = {118000},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118000},
% url = {https://www.sciencedirect.com/science/article/pii/S0957417422012234},
author = {M. {Oliveira} and Eurico Farinha Pedrosa and André Silva Pinto {de Aguiar} and Daniela Ferreira Pinto Dias Rato and Filipe Baptista Neves {dos Santos} and Paulo Miguel {de Jesus Dias} and Vítor Manuel Ferreira {dos Santos}},
keywords = {Extrinsic calibration, Intrinsic calibration, Registration, Multi-modal, Multi-sensor, ROS},
abstract = {The fusion of data from different sensors often requires that an accurate geometric transformation between the sensors is known. The procedure by which these transformations are estimated is known as sensor calibration. The vast majority of calibration approaches focus on specific pairwise combinations of sensor modalities, unsuitable to calibrate robotic systems containing multiple sensors of varied modalities. This paper presents a novel calibration methodology which is applicable to multi-sensor, multi-modal robotic systems. The approach formulates the calibration as an extended optimization problem, in which the poses of the calibration patterns are also estimated. It makes use of a topological representation of the coordinates frames in the system, in order to recalculate the poses of the sensors throughout the optimization. Sensor poses are retrieved from the combination of geometric transformations which are atomic, in the sense that they are indivisible. As such, we refer to this approach as ATOM - Atomic Transformations Optimization Method. This makes the approach applicable to different calibration problems, such as sensor to sensor, sensor in motion, or sensor to coordinate frame. Additionally, the proposed approach provides advanced functionalities, integrated into ROS, designed to support the several stages of a complete calibration procedure. Results covering several robotic platforms and a large spectrum of calibration problems show that the methodology is in fact general, and achieves calibrations which are as accurate as the ones provided by state of the art methods designed to operate only for specific combinations of pairwise modalities.}
}

@ARTICLE{Rato2022497,
	author = {Rato, D. and Oliveira, Miguel and Santos, Vítor and Gomes, Manuel and Sappa, Angel},
	title = {A sensor-to-pattern calibration framework for multi-modal industrial collaborative cells},
	year = {2022},
	journal = {Journal of Manufacturing Systems},
	volume = {64},
	pages = {497 – 507},
	doi = {10.1016/j.jmsy.2022.07.006},
	% url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135711094&doi=10.1016%2fj.jmsy.2022.07.006&partnerID=40&md5=78f90634660be196f1d5adcb89d66b92},
	publication_stage = {Final},
}

@book{Mooring1991,
   author = {Benjamin W Mooring and Zvi S Roth and Morris R Oriels and John Wiley and New York and I Chichester and / Brisbane and I Toronto and I Singapore},
   isbn = {0471508640},
   title = {Fundamentals of Manipulator Calibration},
   year = {1991},
}

@article{Li2016,
   author = {Kuan Lin Li and Wu Te Yang and Kuei Yuan Chan and Pei Chun Lin},
   doi = {10.1186/S40064-016-3417-5/FIGURES/10},
   issn = {21931801},
   issue = {1},
   journal = {SpringerPlus},
   keywords = {Geometric tolerance,Optimization,Robot manipulator,Trajectory planning,Uncertainties},
   month = {12},
   pages = {1-16},
   publisher = {SpringerOpen},
   title = {An optimization technique for identifying robot manipulator parameters under uncertainty},
   volume = {5},
  %  url = {https://springerplus.springeropen.com/articles/10.1186/s40064-016-3417-5},
   year = {2016},
}

@article{Stepanova2019,
   author = {Karla Stepanova and Tomas Pajdla and Matej Hoffmann},
   doi = {10.1109/LRA.2019.2898320},
   issn = {23773766},
   issue = {2},
   journal = {IEEE Robotics and Automation Letters},
   keywords = {Humanoid robots,calibration and identification,force and tactile sensing,kinematics,optimization and optimal control},
   month = {4},
   pages = {1900-1907},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Robot Self-Calibration Using Multiple Kinematic Chains-A Simulation Study on the iCub Humanoid Robot},
   volume = {4},
   year = {2019},
}

@inproceedings{abbatematteo2019learning,
  title={Learning to generalize kinematic models to novel objects},
  author={Abbatematteo, Ben and Tellex, Stefanie and Konidaris, George},
  booktitle={Proceedings of the 3rd Conference on Robot Learning},
  year={2019}
}

@article{Li2020,
  %  abstract = {This paper addresses the task of category-level pose estimation for articulated objects from a single depth image. We present a novel category-level approach that correctly accommodates object instances previously unseen during training. We introduce Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) - a canonical representation for different articulated objects in a given category. As the key to achieve intra-category generalization, the representation constructs a canonical object space as well as a set of canonical part spaces. The canonical object space normalizes the object orientation, scales and articulations (e.g. joint parameters and states) while each canonical part space further normalizes its part pose and scale. We develop a deep network based on PointNet++ that predicts ANCSH from a single depth point cloud, including part segmentation, normalized coordinates, and joint parameters in the canonical object space. By leveraging the canonicalized joints, we demonstrate: 1) improved performance in part pose and scale estimations using the induced kinematic constraints from joints; 2) high accuracy for joint parameter estimation in camera space.},
   author = {Xiaolong Li and He Wang and Li Yi and Leonidas J. Guibas and A. Lynn Abbott and Shuran Song},
   doi = {10.1109/CVPR42600.2020.00376},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {3703-3712},
   publisher = {IEEE Computer Society},
   title = {Category-Level Articulated Object Pose Estimation},
   year = {2020},
}

@article{Liu2022,
  %  abstract = {Human life is populated with articulated objects. Current Category-level Articulation Pose Estimation (CAPE) methods are studied under the single-instance setting with a fixed kinematic structure for each category. Considering these limitations, we aim to study the problem of estimating part-level 6D pose for multiple articulated objects with unknown kinematic structures in a single RGB-D image, and reform this problem setting for real-world environments and suggest a CAPE-Real (CAPER) task setting. This setting allows varied kinematic structures within a semantic category, and multiple instances to co-exist in an observation of real world. To support this task, we build an articulated model repository ReArt-48 and present an efficient dataset generation pipeline, which contains Fast Articulated Object Modeling (FAOM) and Semi-Authentic MixEd Reality Technique (SAMERT). Accompanying the pipeline, we build a large-scale mixed reality dataset ReArtMix and a real world dataset ReArtVal. Accompanying the CAPER problem and the dataset, we propose an effective framework that exploits RGB-D input to estimate part-level pose for multiple instances in a single forward pass. In our method, we introduce object detection from RGB-D input to handle the multi-instance problem and segment each instance into several parts. To address the unknown kinematic structure issue, we propose an Articulation Parsing Network to analyze the structure of detected instance, and also build a Pair Articulation Pose Estimation module to estimate per-part 6D pose as well as joint property from connected part pairs. Extensive experiments demonstrate that the proposed method can achieve good performance on CAPER, CAPE and instance-level Robot Arm pose estimation problems. We believe it could serve as a strong baseline for future research on the CAPER task. The datasets and codes in our work will be made publicly available.},
   author = {Liu Liu and Han Xue and Wenqiang Xu and Haoyuan Fu and Cewu Lu},
   doi = {10.1109/TIP.2021.3138644},
   issn = {19410042},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Articulation estimation,Articulation parsing,Category-level 6D pose,Real-world},
   pages = {1072-1083},
   pmid = {34986097},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Toward Real-World Category-Level Articulation Pose Estimation},
   volume = {31},
   year = {2022},
}

@inproceedings{Michel2015,
  %  abstract = {In this paper, we address the problem of one shot pose estimation of articulated ob-jects from an RGB-D image. In particular, we consider object instances with the topol-ogy of a kinematic chain, i.e. assemblies of rigid parts connected by prismatic or revolute joints. This object type occurs often in daily live, for instance in the form of furniture or electronic devices. Instead of treating each object part separately we are using the rela-tionship between parts of the kinematic chain and propose a new minimal pose sampling approach. This enables us to create a pose hypothesis for a kinematic chain consist-ing of K parts by sampling K 3D-3D point correspondences. To asses the quality of our method, we gathered a large dataset containing four objects and 7000+ annotated RGB-D frames 1 . On this dataset we achieve considerably better results than a modified state-of-the-art pose estimation system for rigid objects.},
   author = {Frank Michel and Alexander Krull and Eric Brachmann and Michael Ying Yang and Stefan Gumhold and Carsten Rother},
   doi = {10.5244/C.29.181},
   journal = {BMVC},
   month = {12},
   pages = {181.1-181.11},
   publisher = {British Machine Vision Association and Society for Pattern Recognition},
   title = {Pose Estimation of Kinematic Chain Instances via Object Coordinate Regression},
  %  url = {https://www.researchgate.net/publication/292033202_Pose_Estimation_of_Kinematic_Chain_Instances_via_Object_Coordinate_Regression},
   year = {2015},
}

@thesis{Sturm2011thesis,
   author = {Jürgen Sturm},
   institution = {University of Freiburg},
   title = {Approaches to Probabilistic Model Learning for Mobile Manipulation Robots},
   year = {2011},
}

@article{Sturm2011,
  %  abstract = {<p>Robots operating in domestic environments generally need to interact with articulated objects, such as doors, cabinets, dishwashers or fridges. In this work, we present a novel, probabilistic framework for modeling articulated objects as kinematic graphs. Vertices in this graph correspond to object parts, while edges between them model their kinematic relationship. In particular, we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations. We furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks. We finally present how the learned models can be generalized to new and previously unseen objects. In various experiments using real robots with different camera systems as well as in simulation, we show that our approach is valid, accurate and efficient. Further, we demonstrate that our approach has a broad set of applications, in particular for the emerging fields of mobile manipulation and service robotics.</p>},
   author = {J. Sturm and C. Stachniss and W. Burgard},
   doi = {10.1613/JAIR.3229},
   issn = {1076-9757},
   journal = {Journal of Artificial Intelligence Research},
   month = {8},
   pages = {477-526},
   title = {A Probabilistic Framework for Learning Kinematic Models of Articulated Objects},
   volume = {41},
   url = {https://jair.org/index.php/jair/article/view/10718},
   year = {2011},
}

@article{Katz2014,
  %  abstract = {We present a skill for the perception of three-dimensional kinematic structures of rigid articulated bodies with revolute and prismatic joints. The ability to acquire such models autonomously is required for general manipulation in unstructured environments. Experiments on a mobile manipulation platform with real-world objects under varying lighting conditions demonstrate the robustness of the proposed method. This robustness is achieved by integrating perception and manipulation capabilities: the manipulator interacts with the environment to move an unknown object, thereby creating a perceptual signal that reveals the kinematic properties of the object. For good performance, the perceptual skill requires the presence of trackable visual features in the scene.},
   author = {Dov Katz and Andreas Orthey and Oliver Brock},
   doi = {10.1007/978-3-642-28572-1_21},
   isbn = {9783642285714},
   issn = {1610742X},
   journal = {Springer Tracts in Advanced Robotics},
   pages = {301-315},
   publisher = {Springer Verlag},
   title = {Interactive perception of articulated objects},
   volume = {79},
   year = {2014},
}

@article{MartinMartin2022,
  %  abstract = {We present online multi-modal perception systems for extracting kinematic and dynamic models of articulated objects from physical interactions with the environment. The systems rely on a RGB-D stream, contact wrenches, and proprioception. The proposed systems share an algorithmic foundation: they are based on an architecture of coupled recursive estimation processes. We present and advocate this architecture as a general, versatile, and robust solution for online interactive perception problems. We validate the architecture in extensive experiments to extract kinematic models interactively, varying the appearance, size, structure, and dynamic properties of objects for different tasks and under different environmental conditions. In addition, we experimentally show that the information acquired by the online perception systems enables robot manipulation of articulated objects. Furthermore, we discuss the relationship between the proposed architecture for robot perception and insights about biological perception systems.},
   author = {Roberto Martín-Martín and Oliver Brock},
   doi = {10.1177/0278364919848850},
   issn = {17413176},
   issue = {8},
   journal = {International Journal of Robotics Research},
   keywords = {Interactive perception,articulated objects,multimodal perception,robot manipulation,visual perception},
   month = {7},
   pages = {741-777},
   publisher = {SAGE Publications Inc.},
   title = {Coupled recursive estimation for online interactive perception of articulated objects},
   volume = {41},
   year = {2022},
}

@article{Hausman2015,
  %  abstract = {We introduce a particle filter-based approach to representing and actively reducing uncertainty over articulated motion models. The presented method provides a probabilistic model that integrates visual observations with feedback from manipulation actions to best characterize a distribution of possible articulation models. We evaluate several action selection methods to efficiently reduce the uncertainty about the articulation model. The full system is experimentally evaluated using a PR2 mobile manipulator. Our experiments demonstrate that the proposed system allows for intelligent reasoning about sparse, noisy data in a number of common manipulation scenarios.},
   author = {Karol Hausman and Scott Niekum and Sarah Osentoski and Gaurav S. Sukhatme},
   doi = {10.1109/ICRA.2015.7139655},
   isbn = {9781479969234},
   issn = {10504729},
   issue = {June},
   journal = {Proceedings - IEEE International Conference on Robotics and Automation},
   month = {6},
   pages = {3305-3312},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Active articulation model estimation through interactive perception},
   volume = {2015-June},
   year = {2015},
}
